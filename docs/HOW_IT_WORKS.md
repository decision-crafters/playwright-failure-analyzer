# 🎓 How the Playwright Failure Analyzer Works

**A comprehensive guide for developers and AI assistants**

This document explains how the Playwright Failure Analyzer works, where to place files, and how to integrate it into your repository.

---

## 📚 Table of Contents

1. [Quick Overview](#quick-overview)
2. [File Structure & Placement](#file-structure--placement)
3. [How the Action Works (Step-by-Step)](#how-the-action-works-step-by-step)
4. [How AI Analysis Works](#how-ai-analysis-works)
5. [Configuration Guide](#configuration-guide)
6. [Common Workflows](#common-workflows)
7. [Troubleshooting](#troubleshooting)

---

## 🎯 Quick Overview

### What This Tool Does

```
┌─────────────────┐
│  Playwright     │
│  Test Run       │  (Your tests run)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  JSON Report    │  (Playwright generates test-results.json)
│  Generated      │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  This Action    │  (Parses failures)
│  Analyzes       │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  AI Analysis    │  (Optional: AI analyzes patterns)
│  (Optional)     │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  GitHub Issue   │  (Beautiful formatted issue created)
│  Created        │
└─────────────────┘
```

### Key Benefits

- ✅ **Automatic**: Runs in GitHub Actions, no manual work
- ✅ **Intelligent**: AI-powered insights (optional)
- ✅ **Organized**: One issue per test run with all failures
- ✅ **Actionable**: Clear error messages and suggestions
- ✅ **Cost-effective**: ~$0.0003 per analysis with DeepSeek

---

## 📁 File Structure & Placement

### Where to Save Files in Your Repository

```
your-repository/
├── .github/
│   └── workflows/
│       └── playwright-tests.yml    ← CREATE THIS (GitHub Action workflow)
│
├── tests/                          ← Your Playwright tests (you already have these)
│   ├── login.spec.js
│   ├── dashboard.spec.js
│   └── checkout.spec.js
│
├── playwright.config.js            ← Your Playwright config (you already have this)
│
└── test-results.json              ← GENERATED by workflow (don't create manually)
```

### File Responsibilities

| File | Who Creates It | Purpose |
|------|---------------|---------|
| `.github/workflows/playwright-tests.yml` | **You** (once) | Tells GitHub when to run tests |
| `test-results.json` | **Playwright** (auto) | Contains test results in JSON format |
| **GitHub Issue** | **This Action** (auto) | Displays failures in beautiful format |

---

## 🔄 How the Action Works (Step-by-Step)

### Step 1: Tests Run and Generate Report

**What happens:**
```bash
npx playwright test --reporter=json > test-results.json
```

**Result:** Creates `test-results.json` with structure:
```json
{
  "suites": [
    {
      "title": "Login Tests",
      "specs": [
        {
          "title": "should login successfully",
          "ok": false,
          "tests": [
            {
              "status": "failed",
              "errors": [{"message": "Expected button to be visible"}]
            }
          ]
        }
      ]
    }
  ]
}
```

### Step 2: Action Parses Report

**What the action does:**
1. Reads `test-results.json`
2. Extracts all failed tests
3. Gathers error messages, stack traces, file paths
4. Limits to configured max failures (default: 3)
5. Creates a summary JSON

**Internal processing:**
```python
# Simplified version of what happens
parser = PlaywrightReportParser('test-results.json')
summary = parser.parse_failures(max_failures=3)
# summary contains: failed tests, error messages, stack traces, etc.
```

### Step 3: AI Analysis (Optional)

**If enabled (`ai-analysis: true`):**

1. **Sends to AI:** Test failures + context
2. **AI analyzes:** Patterns, root causes, common issues
3. **Returns:** Summary, root cause analysis, suggested actions

**What AI receives:**
```json
{
  "failures": [
    {"test_name": "Login test", "error": "Button not visible"},
    {"test_name": "Dashboard test", "error": "Timeout waiting for element"}
  ],
  "metadata": {
    "total_tests": 50,
    "failed_tests": 2,
    "browser": "chromium"
  }
}
```

**What AI returns:**
```json
{
  "summary": "Two tests failed due to UI element visibility issues",
  "root_cause_analysis": "Elements may not be fully loaded before assertions",
  "suggested_actions": [
    "Add explicit wait conditions",
    "Increase default timeout",
    "Check async loading behavior"
  ],
  "confidence": 85
}
```

### Step 4: Issue Creation

**The action creates a GitHub issue with:**

```markdown
# 🚨 Playwright Test Failures Detected

## 📊 Test Run Summary
- Total Tests: 50
- Passed: 48 ✅
- Failed: 2 ❌

---

## ❌ Failed Tests

### 1. Login test
**File:** tests/login.spec.js:15
**Error:** Button not visible
**Stack Trace:** ...

---

## 🤖 AI Analysis (if enabled)

**Summary:** Two tests failed due to UI element visibility issues

**Root Cause:** Elements may not be fully loaded before assertions

**💡 Suggested Actions:**
1. Add explicit wait conditions
2. Increase default timeout
3. Check async loading behavior

---

**Confidence:** 85%
```

---

## 🤖 How AI Analysis Works

### Architecture

```
┌──────────────────┐
│  Test Failures   │
│  + Metadata      │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  LiteLLM         │  (Library that talks to multiple AI providers)
│  (Router)        │
└────────┬─────────┘
         │
         ├─────────────┬─────────────┬─────────────┐
         ▼             ▼             ▼             ▼
    ┌────────┐   ┌────────┐   ┌────────┐   ┌────────┐
    │OpenAI  │   │Claude  │   │OpenRouter│ │DeepSeek│
    │GPT-4o  │   │3.5     │   │100+ models│ │Chat   │
    └────────┘   └────────┘   └────────┘   └────────┘
```

### AI Providers Explained

| Provider | Cost/1000 analyses | Best For | Setup |
|----------|-------------------|----------|-------|
| **DeepSeek** (via OpenRouter) | ~$0.30 | Budget-conscious | Easiest |
| **OpenAI GPT-4o-mini** | ~$0.30 | Good balance | Easy |
| **OpenAI GPT-4o** | ~$5.00 | High quality | Easy |
| **Claude 3.5** | ~$6.00 | Premium insights | Medium |

### What AI Sees

**The AI receives this context:**

```json
{
  "test_failures": [
    {
      "test_name": "User can checkout with items in cart",
      "file_path": "tests/checkout.spec.js",
      "line_number": 45,
      "error_message": "Timeout 30000ms exceeded waiting for selector 'button#checkout'",
      "stack_trace": "...",
      "duration": 32.5,
      "browser": "chromium"
    }
  ],
  "metadata": {
    "total_tests": 100,
    "failed_tests": 3,
    "passed_tests": 97,
    "project": "my-app",
    "commit": "abc123",
    "branch": "main"
  }
}
```

**The AI is prompted to:**

1. **Analyze patterns:** Are multiple tests failing in the same way?
2. **Identify root causes:** What's the underlying issue?
3. **Suggest fixes:** Specific, actionable steps
4. **Estimate confidence:** How sure is the AI about its analysis?

**Example AI Response:**

```json
{
  "summary": "3 checkout-related tests are timing out waiting for UI elements",
  "root_cause_analysis": "The checkout button selector may have changed, or the element is taking longer than 30 seconds to appear. This suggests either a slow page load or a selector mismatch.",
  "error_patterns": [
    "All 3 failures involve timeout on selector wait",
    "All 3 are in checkout flow",
    "Similar timeout duration (30-32 seconds)"
  ],
  "suggested_actions": [
    "Verify the selector 'button#checkout' still exists in the DOM",
    "Check if recent code changes modified button IDs/classes",
    "Increase timeout for checkout tests: { timeout: 60000 }",
    "Add retry logic for flaky selectors",
    "Check network tab for slow API calls blocking UI"
  ],
  "confidence": 85,
  "model": "deepseek-chat"
}
```

### When to Use AI Analysis

**✅ Use AI when:**
- Multiple tests are failing
- Failures seem related or patterned
- You want insights on root causes
- You need quick triage suggestions
- Budget allows (~$0.0003 per analysis)

**❌ Skip AI when:**
- Single obvious test failure
- Zero budget for API calls
- Failures are already well-understood
- Running tests very frequently (thousands per day)

---

## ⚙️ Configuration Guide

### Minimal Configuration (No AI)

```yaml
# .github/workflows/playwright-tests.yml
name: E2E Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      issues: write  # REQUIRED

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '18'

      - run: npm ci
      - run: npx playwright install --with-deps

      - name: Run tests
        run: npx playwright test --reporter=json > test-results.json
        continue-on-error: true

      - name: Analyze failures
        if: failure()
        uses: decision-crafters/playwright-failure-analyzer@v1
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
```

### Full Configuration (With AI)

```yaml
# .github/workflows/playwright-tests.yml
name: E2E Tests with AI

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      issues: write

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '18'

      - run: npm ci
      - run: npx playwright install --with-deps

      - name: Run tests
        run: npx playwright test --reporter=json > test-results.json
        continue-on-error: true

      - name: Analyze failures with AI
        if: failure()
        uses: decision-crafters/playwright-failure-analyzer@v1
        with:
          # Required
          github-token: ${{ secrets.GITHUB_TOKEN }}

          # Optional - customize these
          report-path: 'test-results.json'      # Default
          max-failures: 5                        # Default: 3
          issue-title: 'E2E Failures - ${{ github.ref_name }}'
          issue-labels: 'bug,e2e,needs-triage'
          assignees: 'qa-lead,dev-lead'
          deduplicate: true                      # Default: true
          ai-analysis: true                      # Enable AI

        env:
          # AI Configuration
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          AI_MODEL: 'openrouter/deepseek/deepseek-chat'
```

### Setting Up AI (Step-by-Step)

#### Option 1: OpenRouter (Recommended - Cheapest)

1. **Get API Key**
   - Go to https://openrouter.ai/
   - Sign up for free account
   - Navigate to Keys section
   - Create new API key
   - Copy the key (starts with `sk-or-...`)

2. **Add to GitHub Secrets**
   ```
   Repository → Settings → Secrets and variables → Actions → New repository secret

   Name: OPENROUTER_API_KEY
   Value: sk-or-v1-... (your actual key)
   ```

3. **Use in Workflow**
   ```yaml
   env:
     OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
     AI_MODEL: 'openrouter/deepseek/deepseek-chat'
   ```

#### Option 2: OpenAI

1. **Get API Key**
   - Go to https://platform.openai.com/
   - Navigate to API keys
   - Create new secret key
   - Copy the key (starts with `sk-...`)

2. **Add to GitHub Secrets**
   ```
   Name: OPENAI_API_KEY
   Value: sk-... (your actual key)
   ```

3. **Use in Workflow**
   ```yaml
   env:
     OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
     AI_MODEL: 'gpt-4o-mini'  # or 'gpt-4o'
   ```

---

## 📋 Common Workflows

### Workflow 1: Basic E2E Testing

**Use case:** Run tests on every push, create issue if failures occur

```yaml
name: E2E Tests
on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      issues: write
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm ci && npx playwright install --with-deps
      - run: npx playwright test --reporter=json > test-results.json
        continue-on-error: true
      - if: failure()
        uses: decision-crafters/playwright-failure-analyzer@v1
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
```

### Workflow 2: PR Blocking with Failure Report

**Use case:** Block PRs if tests fail, provide detailed report

```yaml
name: PR Tests
on: pull_request

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm ci && npx playwright install --with-deps

      - name: Run tests
        id: tests
        run: npx playwright test --reporter=json > test-results.json
        continue-on-error: true

      - name: Analyze failures
        if: failure()
        id: analyze
        uses: decision-crafters/playwright-failure-analyzer@v1
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          deduplicate: false  # Always create for PRs

      - name: Comment on PR
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `⚠️ Tests failed. See issue #${{ steps.analyze.outputs.issue-number }} for details.`
            })

      - name: Fail if tests failed
        if: steps.tests.outcome == 'failure'
        run: exit 1
```

### Workflow 3: Scheduled Testing with AI

**Use case:** Nightly tests with AI analysis

```yaml
name: Nightly E2E
on:
  schedule:
    - cron: '0 2 * * *'  # 2 AM daily

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      issues: write
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - run: npm ci && npx playwright install --with-deps
      - run: npx playwright test --reporter=json > test-results.json
        continue-on-error: true

      - if: failure()
        uses: decision-crafters/playwright-failure-analyzer@v1
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          issue-title: '🌙 Nightly Test Failures - ${{ github.run_number }}'
          issue-labels: 'nightly,automated,e2e'
          assignees: 'qa-team'
          ai-analysis: true
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          AI_MODEL: 'openrouter/deepseek/deepseek-chat'
```

---

## 🔍 Troubleshooting

### Issue: "Playwright report file not found"

**Cause:** Mismatch between where report is created and where action looks

**Solution:**
```yaml
# Make sure these match!
- run: npx playwright test --reporter=json > test-results.json
                                              ↑
                                              This filename...

- uses: decision-crafters/playwright-failure-analyzer@v1
  with:
    report-path: 'test-results.json'  ← ...matches this!
```

**Common mistakes:**
```yaml
# ❌ Wrong - report created in root, action looks in subdirectory
- run: npx playwright test --reporter=json > results.json
- with:
    report-path: 'test-results/results.json'

# ✅ Correct - both point to same location
- run: npx playwright test --reporter=json > test-results.json
- with:
    report-path: 'test-results.json'
```

### Issue: "Permission denied creating issue"

**Cause:** Missing `issues: write` permission

**Solution:**
```yaml
jobs:
  test:
    permissions:
      issues: write  # ← Add this!
```

### Issue: "AI analysis not working"

**Causes & Solutions:**

1. **No API key set**
   ```yaml
   # ❌ Missing
   env:
     # No API key!

   # ✅ Fixed
   env:
     OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
   ```

2. **Wrong secret name**
   ```yaml
   # ❌ Typo in secret name
   env:
     OPENROUTER_API_KEY: ${{ secrets.OPEN_ROUTER_KEY }}

   # ✅ Fixed - matches GitHub secret name exactly
   env:
     OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
   ```

3. **AI disabled**
   ```yaml
   # ❌ AI explicitly disabled
   with:
     ai-analysis: false

   # ✅ Fixed
   with:
     ai-analysis: true
   ```

### Issue: "Tests pass but action doesn't run"

**Cause:** Action only runs on failures

**Solution:** This is expected behavior!
```yaml
# The action only runs when tests fail
- if: failure()  # ← Only triggers on test failure
  uses: decision-crafters/playwright-failure-analyzer@v1
```

To run always (for testing):
```yaml
- if: always()  # ← Runs whether tests pass or fail
  uses: decision-crafters/playwright-failure-analyzer@v1
```

---

## 📚 Additional Resources

- **Full API Reference:** [docs/INPUTS.md](INPUTS.md) | [docs/OUTPUTS.md](OUTPUTS.md)
- **Example Workflows:** [examples/](../examples/)
- **AI Testing Guide:** [docs/AI_TESTING_GUIDE.md](AI_TESTING_GUIDE.md)
- **Testing Instructions:** [docs/TESTING_INSTRUCTIONS.md](TESTING_INSTRUCTIONS.md)

---

## 🤝 For AI Assistants

When helping developers integrate this action:

1. **Always include `permissions: issues: write`** in the workflow
2. **Match report filename** between test command and `report-path` input
3. **Use `continue-on-error: true`** on the test step so action can run
4. **Use `if: failure()`** so action only runs when tests fail
5. **For AI analysis**, ensure API key is in GitHub Secrets and passed via `env:`
6. **Recommend `@v1`** tag for most users (stable, gets bug fixes)

**Quick checklist for validation:**
```yaml
✅ permissions: issues: write present
✅ Test generates JSON: --reporter=json
✅ Test has continue-on-error: true
✅ Action has if: failure() or if: always()
✅ report-path matches output filename
✅ Using @v1 or @v1.x.x (not @main for production)
✅ If AI enabled: API key secret configured
```

---

**Happy Testing!** 🎭✨
