# Local Features Implementation Summary

**Status**: ‚úÖ Complete
**Version**: 1.2.0 (Auto-Fix Ready)
**Date**: 2025-10-17

---

## Overview

The Playwright Failure Analyzer has been enhanced with comprehensive auto-fix capabilities, providing three tiers of automation for teams of all sizes:

1. **Issue-Only Mode** - AI-generated fix suggestions in issues (no code changes)
2. **Branch Mode** - Automatic branch creation with suggested fixes (safe testing)
3. **Dagger Integration** - Full automation with external tooling (advanced)

---

## ‚ú® New Features Implemented

### 1. Model-Based Confidence Scoring

**Location**: `src/ai_analysis.py`

Confidence scores are now automatically adjusted based on the AI model tier:

```python
MODEL_CONFIDENCE_MULTIPLIERS = {
    # Premium models (GPT-4o, Claude 3.5 Sonnet)
    'premium': 1.0,     # No adjustment

    # Balanced models (GPT-4o-mini, Claude 3.5 Haiku)
    'balanced': 0.85,   # 15% reduction

    # Budget models (DeepSeek, Llama)
    'budget': 0.70,     # 30% reduction

    # Unknown/basic models
    'basic': 0.60,      # 40% reduction
}
```

**Why this matters**: Budget models report 80% confidence ‚Üí Final confidence is 56% (below auto-fix threshold of 70%)

**Output in issues**:
```markdown
*Analysis generated by openrouter/deepseek/deepseek-chat (budget tier) - Confidence: 56% (raw: 80%)*
```

---

### 2. Fixability Scoring

**Location**: `src/ai_analysis.py`

Each failure receives a fixability score indicating auto-fix suitability:

| Score | Classification | AI Assessment |
|-------|----------------|---------------|
| 0.9-1.0 | Trivial | Missing await, simple typos |
| 0.7-0.89 | Easy | Selectors, timeouts, types |
| 0.5-0.69 | Moderate | Setup/teardown, timing |
| 0.3-0.49 | Complex | Business logic, race conditions |
| 0.0-0.29 | Not fixable | Domain knowledge required |

**Output in issues**:
```markdown
*üü¢ Auto-fix feasibility: 85%*
```

---

### 3. Structured JSON Export

**Location**: `src/parse_report.py`

Machine-readable failure data optimized for automation:

```bash
python src/parse_report.py \
  --report-path playwright-report/results.json \
  --export-structured-json \
  --structured-json-path failures.json
```

**Output format**:
```json
{
  "version": "1.0",
  "format": "playwright-failure-analyzer-structured",
  "failures": [
    {
      "test_name": "Login should work",
      "file_path": "tests/auth.spec.ts",
      "line_number": 42,
      "error_type": "timeout",
      "error_message": "...",
      "fixability_hint": "high - check element selector matches DOM",
      "suggested_pattern": "selector_timeout"
    }
  ],
  "auto_fix_context": {
    "repository": "owner/repo",
    "sha": "abc123",
    "branch": "main"
  }
}
```

---

### 4. Auto-Fix Labels

**Location**: `src/create_issue.py`

Issues are automatically labeled based on analysis:

| Label | Condition |
|-------|-----------|
| `auto-fix-ready` | Fixability ‚â• 75% |
| `high-fixability` | Fixability ‚â• 75% |
| `auto-fix-candidate` | Fixability 50-74% |
| `medium-fixability` | Fixability 50-74% |
| `ai-tier-{tier}` | Model tier used |
| `pattern-{pattern}` | Error pattern detected |

**Example**:
- Issue for selector timeout with 85% fixability
- Labels: `auto-fix-ready`, `high-fixability`, `ai-tier-balanced`, `pattern-selector-timeout`

---

### 5. Auto-Fix Mode (NEW!)

**Location**: `src/auto_fix.py`, `src/create_issue.py`

Three modes for different team needs:

#### Mode 1: `issue-only` (Safest)

**What it does**: Adds AI-generated fix code to the issue in a collapsed section

**Usage**:
```yaml
- uses: decision-crafters/playwright-failure-analyzer@v1
  with:
    auto-fix-mode: 'issue-only'
    ai-analysis: true
```

**Result in issue**:
```markdown
<details>
<summary>ü§ñ AI-Generated Fix Suggestions (Click to expand)</summary>

## Fix 1/1

### üìù tests/sample.spec.ts:42

**Pattern**: `selector_timeout`
**Confidence**: üü¢ 85%

**Original code:**
```typescript
await page.click('.submit-button')
```

**Suggested fix:**
```typescript
await page.click('.submit-button', { timeout: 30000 })
```

**Reasoning**: Added explicit timeout to prevent timeout errors

</details>
```

**Developer workflow**:
1. Review fix in issue
2. Copy suggested code
3. Test locally
4. Commit if it works

---

#### Mode 2: `branch` (Recommended)

**What it does**: Creates a branch with the suggested fix automatically

**Usage**:
```yaml
- uses: decision-crafters/playwright-failure-analyzer@v1
  with:
    auto-fix-mode: 'branch'
    ai-analysis: true
```

**Result**:
1. Issue created with analysis
2. Branch created: `autofix/issue-123-selector-timeout`
3. Fix committed to branch
4. Issue updated with branch link

**In the issue**:
```markdown
‚úÖ **Auto-fix branch created**: `autofix/issue-123-selector-timeout`

**To test this fix:**
```bash
git fetch origin autofix/issue-123-selector-timeout
git checkout autofix/issue-123-selector-timeout
npx playwright test  # Run tests to verify
```

If the fix works, create a PR from this branch.
```

**Developer workflow**:
1. Checkout the auto-fix branch
2. Run tests
3. If passing ‚Üí Create PR
4. If failing ‚Üí Manual fix required

---

#### Mode 3: `pr-draft` (Future)

**What it does**: Creates a draft PR with the fix (requires additional implementation)

**Planned behavior**:
1. Create branch with fix
2. Run tests in branch
3. If tests pass ‚Üí Create draft PR
4. Developer reviews and merges

---

### 6. Enhanced AI Prompts

**Location**: `src/ai_analysis.py`

AI now provides:
- Fixability scores per failure
- Error pattern classification
- Auto-fix prompts for tooling
- Specific code hints

**System prompt excerpt**:
```
Required JSON structure:
{
  "specific_fixes": [
    {
      "test": "file.spec.js:42",
      "fix": "Add await keyword",
      "code_hint": "await page.goto(url)",
      "fixability_score": 0.95,
      "error_pattern": "missing_await"
    }
  ],
  "fixability_score": 0.85,
  "auto_fix_prompt": "Detailed instructions for automated tools"
}
```

---

### 7. Machine-Parseable Metadata

**Location**: `src/create_issue.py`

Hidden JSON block in issues for tooling:

```markdown
<!-- AUTO-FIX-METADATA
```json
{
  "version": "1.0",
  "fixability_score": 0.85,
  "confidence_score": 0.78,
  "model_tier": "balanced",
  "error_patterns": ["selector_timeout"],
  "auto_fix_prompt": "...",
  "auto_fixable_issues": [...]
}
```
-->
```

**Use cases**:
- Dagger modules read this for automation
- External tools parse for metrics
- CI/CD pipelines use for decision making

---

## üìã Usage Examples

### Example 1: Conservative Team (Issue-Only)

```yaml
name: Test with AI Suggestions

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run Playwright tests
        id: tests
        run: |
          set +e
          npx playwright test
          TEST_EXIT_CODE=$?
          set -e
          echo "test-failed=$([ $TEST_EXIT_CODE -ne 0 ] && echo 'true' || echo 'false')" >> $GITHUB_OUTPUT
          exit $TEST_EXIT_CODE
        continue-on-error: true

      - name: Analyze failures
        if: steps.tests.outputs.test-failed == 'true'
        uses: decision-crafters/playwright-failure-analyzer@v1
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          ai-analysis: true
          auto-fix-mode: 'issue-only'  # Just add suggestions to issue
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          AI_MODEL: 'openrouter/deepseek/deepseek-chat'
```

**Result**: Developers get AI suggestions in issues, manually apply them

---

### Example 2: Progressive Team (Branch Mode)

```yaml
- name: Analyze with auto-fix branches
  uses: decision-crafters/playwright-failure-analyzer@v1
  with:
    github-token: ${{ secrets.GITHUB_TOKEN }}
    ai-analysis: true
    auto-fix-mode: 'branch'  # Create branches with fixes
    export-structured-json: true  # Also for Dagger if needed
  env:
    OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
    AI_MODEL: 'openai/gpt-4o-mini'  # Balanced model
```

**Result**: Auto-fix branches created, developers test and create PRs

---

### Example 3: Advanced Team (Dagger Integration)

```yaml
- name: Analyze failures
  id: analyze
  uses: decision-crafters/playwright-failure-analyzer@v1
  with:
    github-token: ${{ secrets.GITHUB_TOKEN }}
    ai-analysis: true
    export-structured-json: true  # For Dagger
    auto-fix-mode: 'issue-only'  # Suggestions in issue
  env:
    OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
    AI_MODEL: 'openrouter/deepseek/deepseek-coder'

- name: Auto-fix with Dagger (future)
  if: steps.analyze.outputs.fixability-score >= 0.75
  uses: dagger/dagger-for-github@v6
  with:
    module: github.com/your-org/playwright-auto-fixer
    args: |
      attempt-fix \
        --failures-json=${{ steps.analyze.outputs.structured-json-path }}
```

**Result**: Full automation with Dagger modules (requires demo repo setup)

---

## üéØ Decision Matrix

| Team Type | AI Analysis | Auto-Fix Mode | Export JSON | Best For |
|-----------|-------------|---------------|-------------|----------|
| Conservative | ‚úÖ | `issue-only` | ‚ùå | Teams new to AI assistance |
| Progressive | ‚úÖ | `branch` | ‚úÖ | Teams wanting safe automation |
| Advanced | ‚úÖ | `issue-only` | ‚úÖ | Teams using Dagger/external tools |
| Hybrid | ‚úÖ | `branch` | ‚úÖ | Best of both worlds |

---

## üìä Cost Analysis by Model Tier

### Budget Model (DeepSeek)

```yaml
AI_MODEL: 'openrouter/deepseek/deepseek-chat'
```

- **Cost**: ~$0.30 per 1000 analyses
- **Confidence multiplier**: 0.70x
- **Best for**: Initial analysis, high-volume repos
- **Example**: 80% AI confidence ‚Üí 56% final (below auto-fix threshold)

### Balanced Model (GPT-4o-mini)

```yaml
AI_MODEL: 'openai/gpt-4o-mini'
```

- **Cost**: ~$0.30 per 1000 analyses
- **Confidence multiplier**: 0.85x
- **Best for**: Production use, balanced quality/cost
- **Example**: 80% AI confidence ‚Üí 68% final (needs manual review)

### Premium Model (GPT-4o, Claude 3.5 Sonnet)

```yaml
AI_MODEL: 'openai/gpt-4o'
# or
AI_MODEL: 'anthropic/claude-3.5-sonnet'
```

- **Cost**: ~$5-6 per 1000 analyses
- **Confidence multiplier**: 1.0x
- **Best for**: Critical fixes, low-volume repos
- **Example**: 80% AI confidence ‚Üí 80% final (auto-fix eligible)

### Hybrid Approach (Recommended)

```yaml
# Use budget model for analysis
ANALYSIS_MODEL: 'openrouter/deepseek/deepseek-chat'

# Use premium model only for high-fixability fixes
FIX_MODEL: 'openai/gpt-4o'
```

**Savings**: ~90% cost reduction vs. using premium for everything

---

## üîí Security Considerations

### Automatic Safety Checks

The system automatically blocks auto-fix for:

1. **Sensitive patterns** (detected in error messages):
   - Authentication/authorization code
   - Payment/billing logic
   - Database operations
   - Security-critical code

2. **Low confidence** (< 70% after model adjustment):
   - Falls back to manual suggestions
   - Requires human review

3. **Complex failures** (fixability < 0.5):
   - No automated suggestions
   - Manual investigation required

### Recommended Safeguards

```yaml
# Only auto-fix in test directories
if: startsWith(github.event.issue.labels.*.name, 'pattern-') && contains(failure.file_path, 'tests/')

# Require approvals for auto-fix PRs
reviewers: ${{ github.actor }}, tech-lead

# Run tests before creating PR
- name: Validate fix
  run: npx playwright test ${{ steps.fix.outputs.test-file }}
```

---

## üìà Success Metrics

Track these metrics to optimize your auto-fix setup:

```python
# Example metrics collection
metrics = {
    "total_failures": 100,
    "ai_analyzed": 95,
    "high_fixability": 45,  # ‚â•70%
    "branches_created": 40,
    "successful_fixes": 32,
    "success_rate": 0.80,   # 32/40
    "time_saved_hours": 24,  # 32 fixes √ó 45 min avg
    "cost_spent": "$15.00",  # API costs
    "roi": "96x"  # ($2,400 saved / $15 spent)
}
```

---

##  Next Steps

### ‚úÖ Local Features (Complete)

All analyzer enhancements are done:
- Model-based confidence scoring
- Fixability scoring
- Structured JSON export
- Auto-fix labels
- Issue-only mode
- Branch creation mode
- Enhanced AI prompts

### üîÑ Demo Repo Integration (Next)

See `DEMO_REPO_DAGGER_GUIDE.md` for:
- Dagger module development
- Pattern library expansion
- Full automation workflows
- Reference implementation

### üìä Community Validation (Future)

- Collect success rate data
- Refine pattern library
- Add more error patterns
- Improve confidence scoring

---

## üÜò Troubleshooting

### Issue: Auto-fix mode not working

**Symptoms**: Mode set to 'branch' but no branch created

**Causes**:
1. Fixability score too low (< 70%)
2. AI analysis not enabled
3. Auto-fix module import failed

**Solution**:
```bash
# Check logs for:
"Fixability score too low (XX%) for auto-fix"
"Auto-fix requested but auto_fix module not available"

# Verify settings:
ai-analysis: 'true'
auto-fix-mode: 'branch'
```

### Issue: Low fixability scores

**Cause**: Using budget model reduces confidence

**Solution**: Either:
1. Accept lower confidence (intended behavior)
2. Use balanced/premium model for higher scores
3. Start with `issue-only` mode

### Issue: Branch creation fails

**Cause**: Missing git permissions

**Solution**:
```yaml
permissions:
  contents: write  # Required for branch creation
  issues: write
```

---

## üìö Related Documentation

- [Auto-Fix Integration Guide](./AUTOFIX_INTEGRATION_GUIDE.md) - Complete usage guide
- [Auto-Fix Prompt Templates](./AUTOFIX_PROMPT_TEMPLATES.md) - Reusable prompts
- [Demo Repo Dagger Guide](./DEMO_REPO_DAGGER_GUIDE.md) - Reference implementation (next)
- [NEW_FEATURE.md](/NEW_FEATURE.md) - Full research and business case

---

**Questions?** Open an issue at [decision-crafters/playwright-failure-analyzer](https://github.com/decision-crafters/playwright-failure-analyzer)
